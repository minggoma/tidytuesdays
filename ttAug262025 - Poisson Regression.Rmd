---
title: "tt07Oct2025"
author: "AM"
date: "2026-02-25"
output: html_document
---

# Intro
I wanted to do a Tidy Tuesday dataset that had some sort of a count variable, since I wanted to practice Poisson regression. Luckily for me, there was this dataset, with a measure of the number of weeks at number one, and a whole slew of different variables. 

The goal of this Tidy Tuesday effort will be to clean the data, then explore all of the variables (I want to challenge myself to see if I can do this in an efficient way), and then do a Poisson regression. 

My plan of attack is as follows:
1. Clean the data
2. Explore the data, find variables that may have a relationship with outcome
3. Check my model assumptions
4. Fit the model
5. Check measure of goodness-of-fit of the model

```{r}
tuesdata <- tidytuesdayR::tt_load('2025-08-26')

bb <- tuesdata$billboard
topics <- tuesdata$topics

library(tidyverse)
library(ggplot2)

```
## Cleaning the Data
This dataset has 105 variables - quite a lot. Some of the data can be grouped into general buckets, such as data on the artist, data on the producer, data on the song itself, etc. I'll break up my data cleaning efforts into those major chunks for my own ease. 

The goal for the data cleaning is to convert variables to their correct data type, factorize any dummy variables, combine any variables needed to make the data set tidy, and see if any variables have a large number of missing values. 

I'll write a function below that will return the proportion of values that are NA for a given vector of column names.
```{r}
#-----------------------------------------------------------------------------#
# Function Name: propna()
# Arguments:
#   df - a data frame
#   colvec - a list of column names in df
# Returns: a matrix where the second column is the proportion of NA values in
#   a given column
# Description: Intended for use in data cleaning. Returns the proportion of NA
#   values in a given column within a matrix
#-----------------------------------------------------------------------------#


propna <- function(df, colvec){
    
    #Subset the data frame
    d <- df[, colvec]
    
    #Initialize vector to hold proportions
    props <- c()
    
    for(i in 1:length(colvec)) {
        
        nas <- sum(is.na(d[, i])) / nrow(d)
        
        props <- c(props, nas)
    }
    
    #Return the proportions binded to column names
    return(cbind(colvec, props))
}
```

I'll also write a function to check for any negative values or for any extremely large values for any numeric variables, to check for any potential data entry errors.
```{r}
#-----------------------------------------------------------------------------#
# Function Name: negorout_ind()
# Arguments:
#   vec - a numeric vector
# Returns: a vector of equal length to vec
# Description: Parses through every value of vec. If a value of vec is 0 or 
#   more than 5 standard deviations from the mean, will assign a value of 1
#   to that index in the return vector. Intended for use in negorout()
#-----------------------------------------------------------------------------#

negorout_ind <- function(vec){
    
    #Calculate mean and sd to check outliers
    avg <- mean(vec, na.rm = TRUE)
    std <- sd(vec, na.rm = TRUE)
    
    #Initialize vector for return
    temp <- c()
    
    for(i in 1:length(vec)){
    
        
        #If value is NA, its not negative or an outlier
        if(is.na(vec[i])) {temp <- c(temp, 0)}
        
        #Assign value of 1 if a value is negative or outlier
        else if(vec[i] < 0 | vec[i] > (avg + 5*std)) {temp <- c(temp, 1)}
        
        #Assign value of 0 otherwise
        else{temp <- c(temp, 0)}
        
        #Continuously add values into the vector
    }
    
    
    #Return vector of same length as nrow(df)
    return(temp)
}
```

```{r}
#-----------------------------------------------------------------------------#
# Function Name: negorout()
# Arguments:
#   df - a data frame
#   colvec - a vector of column names in df
# Returns: a matrix of indices where a value is negative or an outlier
# Description: Parses through every column in colvec in df. Applies the function
#   negorout_ind() to determine if any values are negative or outliers. Returns
#   a matrix that contains vectors of indices for negative or outlier values
#   for a given variable
#-----------------------------------------------------------------------------#
negorout <- function(df, colvec){
    
    #Subset data frame
    d <- df[, colvec]
    
    #Initialize empty vector for return
    ret <- c()
    
    for(i in 1:length(colvec)){
        
        #Only progress if column is numeric
        if(is.numeric(d[, i][[1]])){

            sumvec <- negorout_ind(d[, i][[1]])
            
            #If any value is an outlier or negative, cbind the vector
            if(sum(sumvec, na.rm = TRUE) > 0){
                
                ret <- cbind(ret, sumvec) 
                
                #Rename the right-most column to be equal to the variable name
                colnames(ret)[ncol(ret)] <- colvec[i]
                }
            else{}
            
        }
        #If column is not numeric, do nothing
        else{}
    }
    
    return(ret)
}
```


### Section 1

Looking at the data dictionary that's provided on the [Tidy Tuesday github page](https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-08-26/readme.md) for this data set, it looks like basically everything is the right data type, with no dummy variables. As seen below, the proportion of NA values for this chunk is very low. 
```{r}
#List of column names for this chunk
c1 <- names(bb)[1:16]

head(bb %>% select(all_of(c1)), 10)
```

Examining the proportion of NA values, we see that the number of NA values in each column is relatively small. 
```{r}
propna(bb, c1)
```

Additionally, we see that only one column has an outlier or negative value - the variable for number of weeks at the number one ranking. 
```{r}
m <- negorout(bb, c1)

dim(m)

colnames(m)
```
When we examine which value that is:
```{r}

outlierindices <- which(m[, 1] == 1)

bb$weeks_at_number_one[outlierindices]
```
We see that 2 songs were held the top spot for 19 weeks. These songs were:
```{r}
bb$song[outlierindices]
```
Hilariously, I have heard both of these songs. It's interesting because these are both songs that were released within the past 10 years, and both had a very strong presence on TikTok specifically. Fortunately, there is a variable in this dataset that is somewhat related to that (associated_with_dance).

This chunk looks relatively good - it has few NA values, data is already the correct type, and the few outliers that are in this chunk look reasonable and plausible. The only other thing I'd like to do is generate a year variable and a generation variable. The reason I want to do this is because I have a theory that some of the variability in what kinds of songs that rise to the Billboard Top 100 number 1 spot are dependent on the generation. Baby boomers tend to have different music tastes than Gen Alpha, and I think this should be accounted for in the model. The easiest way to do this, in my opinion, is to increment the years into a variable that represents the generation that is dominating the entry-level workforce. For example, right now, that would be Gen Z - which, understandably, listens to songs like Old Town Road and A Bar Song (and apparently, they listen to them a lot). My theory is there is an interaction term with the generation and the genre - Boomers grew up on rock and roll, Gen Z grew up on post-ironic-post-modern rap. 

First, I'll extract the year from the date variable using the year() function, and then use case_when() to create different generations based on the years. The dates span from 1958 to 2025. I'm using [this](https://en.wikipedia.org/wiki/Generation#/media/File:Generation_timeline.svg) reference for the definition of generations in the Western world, and then I'll shift the years by 22 to match when these generations enter what I consider their peak years of cultural influence. 
```{r}
library(lubridate)


bb <- bb %>% mutate(generation = case_when(
    year(date) > 1940 & year(date) <= 1967 ~ "Silent Generation",
    year(date) > 1968 & year(date) <= 1986 ~ "Baby Boomers",
    year(date) > 1987 & year(date) <= 2002 ~ "Generation X",
    year(date) > 2003 & year(date) <= 2018 ~ "Millennials",
    year(date) > 2019 ~ "Gen Z",
    TRUE ~ NA
))

propna(bb, c("generation"))
```

You're welcome to disagree with me on how I've cut up the generations if you want. But the most popular song on June 6, 2008 was Viva La Vida by Coldplay. If you're telling me that's not firmly a Millennial anthem, I'm not really sure what to tell you.

### Section 2
This section contains information mostly about the artist and the producer. There are a few dummy variables that I will factorize after checking the data quality. I won't write as much as I did above, partially since it's getting a little tiring for me, and partially because you've seen what I'm doing.
```{r}
c2 <- names(bb)[17:39]

propna(bb, c2)

m <- negorout(bb, c2)

ncol(m)

#4 columns with outliers

colnames(m)

#The group named after non-lead singer value is mostly 0
#Anything with a value of 1 is getting flagged as an outlier, I'm not concerned
bb$group_named_after_non_lead_singer[which(m[, 1] == 1)]

#Same with posthumous - value is almost always 0
bb$posthumous[which(m[, 2] == 1)]

#Apparently someone was 78 when their song hit #1
bb$front_person_age[which(m[, 3] == 1)]

#Apparently this was Brenda Lee, whose song Rockin' Around the Christmas Tree
# released in 1958 hit the Billboard Hot 100 Number 1 in 2023. Cool! Also, she
# was alive to see it happen. Also cool!
bb[which(m[, 3]==1), ]

#Dummy variable which is intended to have values of 0, 1, 2, or 3
#Value of 30 is likely an entry error, will change it to 3
bb$front_person_age[which(m[, 4] == 1)] <- 3
```

Now I need to factorize all the dummy variables. While the function to check for negative values or outliers is helpful, I also want to make sure that only the correct levels are included for each variable. 
```{r}
#Choose the variables that are dummy variables
dummy <- c2[c(1, 3, 4, 9, 10, 11, 14, 15, 16, 17, 19, 20, 21 , 22, 23)]

for(i in 1:length(dummy)) {
    print(table(bb[, dummy[i]]))
}

#All values look good - now can factorize

for(i in 1:length(dummy)){
    bb[, dummy[i]][[1]] <- factor(bb[, dummy[i]][[1]])
}
```

The only other thing I want to do is combine artist_white and artist_black into one variable - artist_race. I'm doing this because these variables are both related to artist race and are mutually exclusive - an artist cannot be both white and black. I've double checked this below - there are no entries where both artist_white and artist_black are both 1 for the same observation.

```{r}
bb %>% filter(artist_white == 1 & artist_black == 1) %>% nrow()

bb <- bb %>% mutate(artist_race = case_when(
    artist_white == 1 ~ 1,
    artist_black == 2 ~ 2, 
    TRUE ~ 3, #To represent mixed race(s)
))
```
### Section 3

Everything between the previous section and the section after this, which is all dummy variables for whether an instrument appears in a song. 
```{r}
c3 <-names(bb)[40:48]

bb %>% select(all_of(c3)) %>% head()

#Loudness is all negative, I'll just check for outliers by hand

#Very few NA values
propna(bb, c3)

#Remove loudness since it's all negative values
negorout(bb, c3[-8])
#No negative or outlier values

#Looks like there are some extreme values, but seems reasonable and possible
boxplot(bb$loudness_d_b)
```

```{r}
#Just some extra EDA
g <- ggplot(data = bb, aes(x = time_signature)) + geom_bar() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
#The vast majority of songs are in 4/4
g

g <- ggplot(data = bb %>% group_by(keys) %>% summarize(count = n()) %>% filter(count > 5), 
            aes(x = keys, y = count)) + geom_col() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1))
g

#Surprised that so many songs in the minor key are in here
#I'll make a variable for major or minor key

grepl("m", bb$keys)

bb <- bb %>% mutate(major_or_minor = case_when(
    grepl("m", keys) ~ "minor",
    is.na(keys) ~ NA,
    TRUE ~ "major"
))

g <- ggplot(data = bb, aes(x = major_or_minor)) + geom_bar()
g
```

```{r}
#TODO: check levels of each dummy, factorize everything
```


